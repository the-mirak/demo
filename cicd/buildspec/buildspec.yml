version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.9
    commands:
      - echo "Installing dependencies..."
      - curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.23.7/2022-06-29/bin/linux/amd64/kubectl
      - chmod +x kubectl
      - mv kubectl /usr/local/bin/
      - curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/aws-iam-authenticator
      - chmod +x aws-iam-authenticator
      - mv aws-iam-authenticator /usr/local/bin/
      - yum install -y jq gettext
      - echo "Installing eksctl..."
      - curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
      - mv /tmp/eksctl /usr/local/bin
      - eksctl version

  pre_build:
    commands:
      - echo "Setting up environment variables..."
      - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/streamlit-app
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
      - APP_NAME=streamlit-app
      - DOCKERFILE_PATH=Dockerfile
      - ENABLE_CACHE=true
      - echo "Updating kubeconfig..."
      - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_REGION
      - kubectl config view
      - echo "Setting up EKS auth ConfigMap..."
      - chmod +x cicd/scripts/setup-eks-auth.sh
      - |
        ./cicd/scripts/setup-eks-auth.sh --eks-cluster $EKS_CLUSTER_NAME --force || {
          echo "Warning: EKS auth setup failed, but continuing with the build..."
          echo "Attempting to manually update aws-auth ConfigMap..."
          ROLE_ARN=$(aws iam list-roles --query "Roles[?RoleName.contains(@, 'streamlit-app-pipeline') && RoleName.contains(@, 'CodeBuildServiceRole')].Arn" --output text)
          kubectl get configmap aws-auth -n kube-system -o yaml > /tmp/aws-auth.yaml
          if ! grep -q "$ROLE_ARN" /tmp/aws-auth.yaml; then
            echo "Adding CodeBuild role to aws-auth ConfigMap..."
            EXISTING_ROLES=$(kubectl get configmap aws-auth -n kube-system -o jsonpath='{.data.mapRoles}')
            NEW_ROLE="- rolearn: $ROLE_ARN\n  username: codebuild\n  groups:\n  - system:masters"
            UPDATED_ROLES="$EXISTING_ROLES\n$NEW_ROLE"
            kubectl patch configmap aws-auth -n kube-system --type merge -p "{\"data\":{\"mapRoles\":\"$UPDATED_ROLES\"}}"
          fi
        }
      - |
        kubectl get svc || echo "Warning: Could not get services, but continuing with the build..."
      - echo "Creating S3 bucket for context if it doesn't exist..."
      - S3_BUCKET=kaniko-context-$AWS_ACCOUNT_ID-$AWS_REGION
      - aws s3api create-bucket --bucket $S3_BUCKET --region $AWS_REGION --create-bucket-configuration LocationConstraint=$AWS_REGION || true
      - echo "Preparing application context..."
      - tar -czf /tmp/$APP_NAME.tar.gz -C app .
      - aws s3 cp /tmp/$APP_NAME.tar.gz s3://$S3_BUCKET/
      - echo "Running pre-deployment script..."
      - chmod +x cicd/scripts/pre_deploy.sh
      - ./cicd/scripts/pre_deploy.sh

  build:
    commands:
      - echo "Build started on $(date)"
      - echo "Building the container image using Kaniko..."
      - echo "Checking if kaniko-ecr-push-role exists..."
      - ROLE_EXISTS=$(aws iam get-role --role-name kaniko-ecr-push-role 2>/dev/null || echo "false")
      - |
        if [ "$ROLE_EXISTS" != "false" ]; then
          echo "Deleting existing kaniko-ecr-push-role to update trust policy..."
          # Detach policies first
          aws iam detach-role-policy --role-name kaniko-ecr-push-role --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess || true
          aws iam detach-role-policy --role-name kaniko-ecr-push-role --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess || true
          # Delete role
          aws iam delete-role --role-name kaniko-ecr-push-role || true
          # Set ROLE_EXISTS to false to recreate it
          ROLE_EXISTS="false"
          echo "Waiting for role deletion to propagate..."
          sleep 10
        fi
        
        if [ "$ROLE_EXISTS" = "false" ]; then
          echo "Creating kaniko-ecr-push-role for IRSA..."
          # Get OIDC provider ID
          OIDC_PROVIDER_ID=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5)
          
          # Check if OIDC provider exists
          OIDC_PROVIDER_EXISTS=$(aws iam list-open-id-connect-providers | grep $OIDC_PROVIDER_ID || echo "false")
          
          if [ "$OIDC_PROVIDER_EXISTS" = "false" ]; then
            echo "OIDC provider does not exist. Creating it..."
            eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve
          else
            echo "OIDC provider already exists"
          fi
          
          # Create trust policy
          cat > /tmp/trust-policy.json << EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Federated": "arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/oidc.eks.$AWS_REGION.amazonaws.com/id/$OIDC_PROVIDER_ID"
              },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "oidc.eks.$AWS_REGION.amazonaws.com/id/$OIDC_PROVIDER_ID:aud": "sts.amazonaws.com",
                  "oidc.eks.$AWS_REGION.amazonaws.com/id/$OIDC_PROVIDER_ID:sub": "system:serviceaccount:default:kaniko-builder"
                }
              }
            }
          ]
        }
        EOF
          
          # Create role
          aws iam create-role --role-name kaniko-ecr-push-role --assume-role-policy-document file:///tmp/trust-policy.json
          
          # Attach policies
          aws iam attach-role-policy --role-name kaniko-ecr-push-role --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess
          aws iam attach-role-policy --role-name kaniko-ecr-push-role --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
          
          echo "kaniko-ecr-push-role created successfully"
        else
          echo "kaniko-ecr-push-role already exists"
        fi
      - echo "Checking if ECR repositories exist..."
      - |
        # Check if streamlit-app repository exists
        APP_REPO_EXISTS=$(aws ecr describe-repositories --repository-names $APP_NAME 2>/dev/null || echo "false")
        if [ "$APP_REPO_EXISTS" = "false" ]; then
          echo "Creating $APP_NAME ECR repository..."
          aws ecr create-repository --repository-name $APP_NAME
        else
          echo "$APP_NAME ECR repository already exists"
        fi
        
        # Check if kaniko-cache repository exists
        CACHE_REPO_EXISTS=$(aws ecr describe-repositories --repository-names kaniko-cache 2>/dev/null || echo "false")
        if [ "$CACHE_REPO_EXISTS" = "false" ]; then
          echo "Creating kaniko-cache ECR repository..."
          aws ecr create-repository --repository-name kaniko-cache
        else
          echo "kaniko-cache ECR repository already exists"
        fi
      - echo "Applying Kaniko service account..."
      - envsubst < kubernetes/kaniko-service-account.yaml > kubernetes/kaniko-service-account-updated.yaml
      - kubectl apply -f kubernetes/kaniko-service-account-updated.yaml
      - echo "Verifying service account configuration..."
      - kubectl get serviceaccount kaniko-builder -o yaml
      - echo "Creating Kaniko pod..."
      - kubectl delete pod ${APP_NAME}-kaniko-builder --ignore-not-found=true
      - envsubst < kubernetes/kaniko-pod.yaml > kubernetes/kaniko-pod-updated.yaml
      - kubectl apply -f kubernetes/kaniko-pod-updated.yaml
      - echo "Verifying pod configuration..."
      - kubectl get pod ${APP_NAME}-kaniko-builder -o yaml
      - echo "Waiting for Kaniko pod to complete..."
      - kubectl wait --for=condition=Ready pod/${APP_NAME}-kaniko-builder --timeout=60s || true
      - kubectl logs -f ${APP_NAME}-kaniko-builder
      - echo "Waiting for Kaniko build to complete..."
      - until kubectl get pod ${APP_NAME}-kaniko-builder -o jsonpath='{.status.phase}' | grep -q "Succeeded"; do sleep 5; done

  post_build:
    commands:
      - echo "Build completed on $(date)"
      - echo "Update Kubernetes deployment..."
      - envsubst < kubernetes/streamlit-deployment.yaml > kubernetes/streamlit-deployment-updated.yaml
      - kubectl apply -f kubernetes/streamlit-deployment-updated.yaml
      - echo "Waiting for deployment to stabilize..."
      - kubectl rollout status deployment/streamlit-app --timeout=300s
      - echo "Cleaning up Kaniko pod..."
      - kubectl delete pod ${APP_NAME}-kaniko-builder
      - echo "Deployment completed successfully!"
      - echo "Running post-deployment verification..."
      - chmod +x cicd/scripts/post_deploy.sh
      - ./cicd/scripts/post_deploy.sh

artifacts:
  files:
    - kubernetes/streamlit-deployment-updated.yaml
    - kubernetes/kaniko-pod-updated.yaml
    - appspec.yml
    - cicd/scripts/post_deploy.sh
  discard-paths: no